
  0%|                                                                                                                                                                                      | 0/500 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.









  2%|███▍                                                                                                                                                                         | 10/500 [00:26<19:42,  2.41s/it]










  4%|██████▉                                                                                                                                                                      | 20/500 [00:50<18:58,  2.37s/it]


  4%|███████▌                                                                                                                                                                     | 22/500 [00:55<18:52,  2.37s/it]Traceback (most recent call last):
  File "/home/ubuntu/empath/finetune/finetune_falcon2.py", line 92, in <module>
    trainer.save_model("empath-falcon-40b/model")
  File "/home/ubuntu/empath/finetune/venv/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/ubuntu/empath/finetune/venv/lib/python3.10/site-packages/transformers/trainer.py", line 1814, in _inner_training_loop
    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
KeyboardInterrupt
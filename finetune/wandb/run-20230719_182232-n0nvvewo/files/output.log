
  0%|                                                                                                                                                                                       | 0/10 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.








100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:26<00:00,  2.65s/it]
Traceback (most recent call last):
  File "/home/ubuntu/empath/finetune/finetune_falcon2.py", line 92, in <module>
    trainer.save_model("empath-falcon-40b/model")
  File "/home/ubuntu/empath/finetune/venv/lib/python3.10/site-packages/transformers/trainer.py", line 2769, in save_model
    self._save(output_dir)
  File "/home/ubuntu/empath/finetune/venv/lib/python3.10/site-packages/transformers/trainer.py", line 2827, in _save
    self.model.save_pretrained(
  File "/home/ubuntu/empath/finetune/venv/lib/python3.10/site-packages/transformers/modeling_utils.py", line 1715, in save_pretrained
    raise NotImplementedError(
NotImplementedError: You are calling `save_pretrained` on a 4-bit converted model. This is currently not supported
{'loss': 2.1407, 'learning_rate': 0.0002, 'epoch': 2.0}
{'train_runtime': 28.6523, 'train_samples_per_second': 0.349, 'train_steps_per_second': 0.349, 'train_loss': 2.140673828125, 'epoch': 2.0}
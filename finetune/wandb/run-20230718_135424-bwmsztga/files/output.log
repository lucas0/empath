
  0%|                                                                                                                                                                                      | 0/500 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
Traceback (most recent call last):
  File "/home/ubuntu/empath/finetune/finetune_falcon2.py", line 93, in <module>
    trainer.train()
  File "/home/ubuntu/empath/finetune/venv/lib/python3.10/site-packages/transformers/trainer.py", line 1539, in train
    return inner_training_loop(
  File "/home/ubuntu/empath/finetune/venv/lib/python3.10/site-packages/transformers/trainer.py", line 1867, in _inner_training_loop
    self.accelerator.clip_grad_norm_(
  File "/home/ubuntu/empath/finetune/venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 1925, in clip_grad_norm_
    self.unscale_gradients()
  File "/home/ubuntu/empath/finetune/venv/lib/python3.10/site-packages/accelerate/accelerator.py", line 1888, in unscale_gradients
    self.scaler.unscale_(opt)
  File "/home/ubuntu/empath/finetune/venv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 284, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(optimizer, inv_scale, found_inf, False)
  File "/home/ubuntu/empath/finetune/venv/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py", line 212, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.